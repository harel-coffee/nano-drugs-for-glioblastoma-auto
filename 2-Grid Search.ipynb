{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, roc_auc_score,f1_score, recall_score, precision_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LassoCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.feature_selection import RFECV, VarianceThreshold, SelectKBest, chi2\n",
    "from sklearn.feature_selection import SelectFromModel, SelectPercentile, f_classif\n",
    "\n",
    "import feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define output variables\n",
    "outVar = 'Class'\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML with 1 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tr and ts datasets\n",
    "df_tr_std = feather.read_dataframe(r'datasets\\ds.Class.std.tr.feather')\n",
    "df_ts_std = feather.read_dataframe(r'datasets\\ds.Class.std.ts.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for tr and ts\n",
    "X_tr_std = df_tr_std.drop(outVar, axis=1).values\n",
    "y_tr_std = df_tr_std[outVar].values\n",
    "X_ts_std = df_ts_std.drop(outVar, axis=1).values\n",
    "y_ts_std = df_ts_std[outVar].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_samples'  : [0.1, 0.5, 1.0],\n",
    "    'n_estimators' : [5, 10, 20, 50]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = BaggingClassifier(base_estimator=DecisionTreeClassifier(), random_state=seed)\n",
    "gs = GridSearchCV(estimator=cls,\n",
    "                   param_grid=params, n_jobs=-1, verbose=10, scoring ='roc_auc', cv=3)\n",
    "\n",
    "gs.fit(X_tr_std, y_tr_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = gs.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(gs.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = gs.cv_results_['mean_test_score']\n",
    "stds = gs.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, gs.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "print()\n",
    "y_true, y_pred = y_ts_std, gs.predict(X_ts_std)\n",
    "cls_rep = classification_report(y_true, y_pred,target_names=['0','1'],\n",
    "                               output_dict=True, digits=3)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()\n",
    "\n",
    "y_probs = gs.predict_proba(X_ts_std)[:, 1]\n",
    "ACC       = accuracy_score(y_ts_std, y_pred)\n",
    "AUROC     = roc_auc_score(y_ts_std, y_probs)\n",
    "precision = cls_rep['weighted avg']['precision']\n",
    "recall    = cls_rep['weighted avg']['recall']\n",
    "f1score   = cls_rep['weighted avg']['f1-score']\n",
    "\n",
    "print('ACC       = {0:0.3f}'.format(ACC))\n",
    "print('AUROC     = {0:0.3f}'.format(AUROC))\n",
    "print('precision = {0:0.3f}'.format(precision))\n",
    "print('recall    = {0:0.3f}'.format(recall))\n",
    "print('f1score   = {0:0.3f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsResults_df = pd.DataFrame(columns=['Best Grid Search', 'ACC','AUROC' ,'precision' ,'recall' ,'f1-score' ])\n",
    "gsResults_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsResults_df = gsResults_df.append({'Best Grid Search': str(gs.best_params_),\n",
    "                          'ACC': float(ACC),\n",
    "                          'AUROC': float(AUROC),\n",
    "                          'precision': float(precision),\n",
    "                          'recall': float(recall),\n",
    "                          'f1-score': float(f1score)}, ignore_index=True)\n",
    "gsResults_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GS using function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myGridSearch(gs, params, df_tr_std, df_ts_std):\n",
    "    # get data for tr and ts\n",
    "    X_tr_std = df_tr_std.drop(outVar, axis=1).values\n",
    "    y_tr_std = df_tr_std[outVar].values\n",
    "    X_ts_std = df_ts_std.drop(outVar, axis=1).values\n",
    "    y_ts_std = df_ts_std[outVar].values\n",
    "    \n",
    "    gs.fit(X_tr_std, y_tr_std)\n",
    "    \n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(gs.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = gs.cv_results_['mean_test_score']\n",
    "    stds = gs.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, gs.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    \n",
    "    print(\"Detailed classification report:\\n\")\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\\n\")\n",
    "    y_true, y_pred = y_ts_std, gs.predict(X_ts_std)\n",
    "    cls_rep = classification_report(y_true, y_pred,target_names=['0','1'],\n",
    "                                   output_dict=True, digits=3)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    y_probs = gs.predict_proba(X_ts_std)[:, 1]\n",
    "    ACC       = accuracy_score(y_ts_std, y_pred)\n",
    "    AUROC     = roc_auc_score(y_ts_std, y_probs)\n",
    "    precision = cls_rep['weighted avg']['precision']\n",
    "    recall    = cls_rep['weighted avg']['recall']\n",
    "    f1score   = cls_rep['weighted avg']['f1-score']\n",
    "\n",
    "    print('ACC       = {0:0.3f}'.format(ACC))\n",
    "    print('AUROC     = {0:0.3f}'.format(AUROC))\n",
    "    print('precision = {0:0.3f}'.format(precision))\n",
    "    print('recall    = {0:0.3f}'.format(recall))\n",
    "    print('f1score   = {0:0.3f}'.format(f1score))\n",
    "\n",
    "    return gs, ACC, AUROC, precision, recall, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tr and ts datasets\n",
    "df_tr_std = feather.read_dataframe(r'datasets\\ds.Class.std.tr.feather')\n",
    "df_ts_std = feather.read_dataframe(r'datasets\\ds.Class.std.ts.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_samples'  : [0.4, 0.5, 0.6, 1.0],\n",
    "    'n_estimators' : [50, 100, 500]\n",
    "    #'base_estimator__max_depth' : [2, 4, None],\n",
    "    #'base_estimator__max_leaf_nodes' : [10,20,None]\n",
    "}\n",
    "cls = BaggingClassifier(base_estimator=DecisionTreeClassifier(), random_state=seed)\n",
    "gs = GridSearchCV(estimator=cls,\n",
    "                   param_grid=params, n_jobs=-1, verbose=10, scoring ='roc_auc', cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs, ACC, AUROC, precision, recall, f1score = myGridSearch(gs, params, df_tr_std, df_ts_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsResults_df = gsResults_df.append({'Best Grid Search': str(gs.best_params_),\n",
    "                          'ACC': float(ACC),\n",
    "                          'AUROC': float(AUROC),\n",
    "                          'precision': float(precision),\n",
    "                          'recall': float(recall),\n",
    "                          'f1-score': float(f1score)}, ignore_index=True)\n",
    "gsResults_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsResults_df.to_csv(r'results\\gs_1ML.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs.best_estimator_\n",
    "feature_importances = np.mean([\n",
    "    tree.feature_importances_ for tree in gs.best_estimator_.estimators_\n",
    "], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_baseline(cls, X_tr, y_tr, X_ts, y_ts, seed=42, classes=['0','1']):\n",
    "    ACC = 0\n",
    "    AUROC = 0\n",
    "    precision = 0 \n",
    "    recall = 0\n",
    "    f1score = 0\n",
    "    \n",
    "    cls_name = type(cls).__name__\n",
    "    \n",
    "    start_time = time.time()\n",
    "    cls.fit(X_tr, y_tr)\n",
    "    print('>', cls_name, \"training: %0.2f mins \" % ((time.time() - start_time)/60))\n",
    "    \n",
    "    # predictions\n",
    "    y_pred  = cls.predict(X_ts)\n",
    "    y_probs = cls.predict_proba(X_ts)[:, 1]\n",
    "    cls_rep = classification_report(y_ts, y_pred, target_names=classes,\n",
    "                                    output_dict=True, digits=3)\n",
    "    print(cls_rep)\n",
    "    \n",
    "    ACC       = accuracy_score(y_ts, y_pred)\n",
    "    AUROC     = roc_auc_score(y_ts, y_probs)\n",
    "    precision = cls_rep['weighted avg']['precision']\n",
    "    recall    = cls_rep['weighted avg']['recall']\n",
    "    f1score   = cls_rep['weighted avg']['f1-score']  \n",
    "    \n",
    "    return ACC, AUROC, precision, recall, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tr and ts datasets\n",
    "df_tr_std = feather.read_dataframe(r'datasets\\ds.Class.std.tr.feather')\n",
    "df_ts_std = feather.read_dataframe(r'datasets\\ds.Class.std.ts.feather')\n",
    "\n",
    "# get data for tr and ts\n",
    "X_tr_std = df_tr_std.drop(outVar, axis=1).values\n",
    "y_tr_std = df_tr_std[outVar].values\n",
    "X_ts_std = df_ts_std.drop(outVar, axis=1).values\n",
    "y_ts_std = df_ts_std[outVar].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classifiers for baseline\n",
    "classifiers = [\n",
    "               BaggingClassifier(random_state=seed, n_estimators= 5, max_samples=0.5),\n",
    "               BaggingClassifier(random_state=seed, n_estimators=10, max_samples=0.5),\n",
    "               BaggingClassifier(random_state=seed, n_estimators=20, max_samples=0.5),\n",
    "               BaggingClassifier(random_state=seed, n_estimators= 5, max_samples=0.6),\n",
    "               BaggingClassifier(random_state=seed, n_estimators=10, max_samples=0.6),\n",
    "               BaggingClassifier(random_state=seed, n_estimators=20, max_samples=0.6),\n",
    "               BaggingClassifier(random_state=seed, n_estimators= 5, max_samples=1.0),\n",
    "               BaggingClassifier(random_state=seed, n_estimators=10, max_samples=1.0),\n",
    "               BaggingClassifier(random_state=seed, n_estimators=20, max_samples=1.0)\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classifiers for baseline\n",
    "classifiers = [\n",
    "               BaggingClassifier(random_state=seed, n_estimators=5),\n",
    "               BaggingClassifier(random_state=seed, n_estimators=4),\n",
    "               BaggingClassifier(random_state=seed, n_estimators=3),\n",
    "               BaggingClassifier(random_state=seed, n_estimators=2)\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for ML baseline\n",
    "df_ML = pd.DataFrame(columns=['Method', 'ACC','AUROC' ,'precision' ,'recall' ,'f1-score' ])\n",
    "df_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit each classifier\n",
    "for cls in classifiers:\n",
    "    print(\"\\n***\", cls)\n",
    "    ACC,AUROC,precision,recall,f1score=ML_baseline(cls, X_tr_std, y_tr_std, X_ts_std, y_ts_std)\n",
    "    df_ML = df_ML.append({'Method': str(type(cls).__name__),\n",
    "                          'ACC': float(ACC),\n",
    "                          'AUROC': float(AUROC),\n",
    "                          'precision': float(precision),\n",
    "                          'recall': float(recall),\n",
    "                          'f1-score': float(f1score)}, ignore_index=True)\n",
    "\n",
    "df_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ML.to_csv(r'results\\baseline_Bagging3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.n_estimators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
